[1]
label = 10K_train
model_type = spert
model_path = bert-base-cased
tokenizer_path = bert-base-cased
train_final = false
epochs = 30
n_iter = 3
k_fold = 5
sim_name = Logit-Norm-0.01
## Calibration
logit_norm = true
logit_norm_temp = 0.01
### Random Entity Replacement, Weighted Sampler, weighted_loss: {None,DRO}
balance_entity_shuffling = false
random_entity_shuffling = false
weighted_sampling = false
weighted_loss = None
dro_step_size = 0.01
###
### ent_type_embed : {None,Gold,Predicted,OnlyG,OnlyP}, ent_type_embed_method: {Learned,Fixed}
ent_type_embed = None
ent_type_embed_method = Fixed
ent_type_embed_dim = 768
####
### self-supervised data
semi = false
###
### Bert best practices
freeze_lower_layers = 0
randomize_top_layers = 0
reinit_pooler = false
debiased_adamW = false
###
### Augmentation
aug = false
sanitize = false
filtering = strict
sampling = 1gram
no_samples = 1
extend_val_set = false
###
types_path = data/datasets/10K/10K_types.json
train_batch_size = 2
eval_batch_size = 1
neg_entity_count = 100
neg_relation_count = 100
lr = 5e-5
lr_warmup = 0.1
weight_decay = 0.01
max_grad_norm = 1.0
rel_filter_threshold = 0.4
size_embedding = 25
prop_drop = 0.1
max_span_size = 10
store_predictions = true
store_examples = true
sampling_processes = 4
max_pairs = 1000
final_eval = false
log_path = data/log/
save_path = data/save/
seed = 1994
